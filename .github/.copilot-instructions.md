# DataGEMS Data Model Management Repository API Instructions for Automated Agents

## Agent Communication Style

- **Keep summaries SHORT** - Use bullet points, no verbose explanations
- **After major changes:** Ask about possible issues rather than listing details
- **Offer details on request** - "Need more details?" instead of preemptive walls of text
- **Question first** - After refactoring/big changes, ask: "Any issues? Want me to run tests?"

---

## Table of Contents

1. [Directory & File Structure](#1-directory--file-structure)
2. [Commit Policy](#2-commit-policy)
3. [Project Overview](#3-project-overview)
4. [Development Environment](#4-development-environment)
5. [Data Patterns](#5-data-patterns)
6. [Workflows](#6-workflows)
7. [Code Quality & Testing](#7-code-quality--testing)
8. [API Documentation](#8-api-documentation)
9. [What to Implement Next](#9-what-to-implement-next)
10. [References](#10-references)


## 1. Directory & File Structure

- **Root Directory:** `data-model-management`
- **Main API Code:** `dmm_api/` - Contains the FastAPI application and resources
  - `main.py` - FastAPI application entry point
  - `resources/` - API resource modules (dataset operations, data resolver, query executor)
  - `utilities/` - Utility modules for data processing
    - `json_tools.py` - Single module exposing `validate_jsonld` and `convert_jsonld_to_pgjson` (dict-only)
    - `README.md` - Utilities documentation
  - `data/` - Data storage directory for datasets and query results
- **Tests:** `tests/` - Test suite with sample datasets and test files
  - `dataset/` - Sample dataset JSON files
  - `dataset_profile/` - Sample dataset profile files
  - `dataset_query/` - Sample analytical pattern queries
  - `test_utilities.py` - Tests for utility modules
- **Documentation:** `docs/` - MkDocs documentation
- **Configuration Files:**
  - `pyproject.toml` - Project metadata and dependencies (using `uv`)
  - `.pre-commit-config.yaml` - Pre-commit hooks configuration
  - `mkdocs.yml` - Documentation configuration
  - `Makefile` - Project automation tasks

---

## 2. Commit Policy

- **Virtual Environment:** Always run all commands, scripts, and hooks inside the project Python virtual environment (`.venv`). Activate the venv before running any Python, pip, or tool commands using `uv`.

- **Pre-Commit Checks:** Before making any new changes, always check for:
  - Unstaged or uncommitted changes to crucial config files (especially `.pre-commit-config.yaml`, `pyproject.toml`)
  - Syntax errors in crucial files (YAML, Python, JSON, etc.) to avoid cascading failures

- **Non-Interactive Commands:** When scripting or automating rebases, always use `git rebase --continue --no-edit` to avoid opening an interactive editor (e.g., vim) for commit messages. This ensures a fully non-interactive workflow.

- **Commit Frequency:** Agents must always commit their changes after every significant or logical update. This includes:
  - Any new feature, refactor, or config update
  - Any documentation update or reorganization
  - Any formatting or linting pass (applied by pre-commit hooks)
  - Any bugfix or test addition
  - Any time a pre-commit hook or tool modifies a file, the agent is responsible for staging and committing the result, even if it is a whitespace or formatting fix

- **Documentation Commits:** API documentation (using mkdocs) is generated manually (not by pre-commit). Only commit docs when intentionally regenerated (e.g., before a release or after major code changes).

- **Commit Organization:** Group committed files by topic and the extent of the change:
  - If a change affects multiple topics (e.g., docs and code), prefer separate commits for each topic
  - If a change is extensive (touches many files), group by logical area (e.g., all doc files, all API code, all test files, etc.)

- **Commit Message Format:** Each commit message should include the flag `[BOT]` to indicate it was made by an automated agent.

- **Check Git Status and Diff:** Before committing, always run `git status` and `git diff` to review changes. Use this information to craft an accurate and descriptive commit message.

- **File Cleanup Policy:** When refactoring or consolidating modules:
  - **Never leave re-export shims or compatibility wrappers** unless explicitly instructed by the user
  - **Delete deprecated/obsolete files completely** rather than leaving empty placeholders
  - Default behavior is to keep the codebase clean and force migration to new import paths
  - If backward compatibility is required, the user will explicitly request it
  - Always remove unused files, dead code, and temporary artifacts

---

## 3. Project Overview

- **Purpose:** Data Model & Management Platform API (WP5) - A FastAPI-based service for managing datasets, their profiles, and executing analytical pattern queries. Part of the DataGEMS EOSC project.

- **Core Components:**
  - **FastAPI Application:** REST API providing endpoints for dataset registration, retrieval, profiling, and querying
  - **Dataset Management:** Register, update, and retrieve dataset metadata
  - **Dataset Profiling:** Attach and manage dataset profiles
  - **Query Execution:** Execute analytical pattern queries on datasets using DuckDB
  - **Utilities (json_tools):** Reusable utilities for data format validation and conversion (dict-only API)
    - `validate_jsonld(data: dict, strict: bool = False)`
    - `convert_jsonld_to_pgjson(data: dict, *, include_context: bool = False, generate_ids: bool = True)`
  - **Data Storage:** File-based storage for datasets and query results

- **Key Technologies:**
  - FastAPI (v0.115.14+) - Web API framework
  - DuckDB (v1.3.1+) - Query execution engine
  - Pandas (v2.3.0+) - Data manipulation
  - Uvicorn (v0.35.0+) - ASGI server
  - UV - Python package manager (replaces poetry)

- **Core Artifacts:**
  - Dataset profile JSON files (Croissant JSON-LD format)
  - Analytical pattern query JSON files (PG-JSON format)
  - Query result datasets stored in `dmm_api/data/results/`
  - Dockerfile for containerized deployment

---

## 4. Development Environment

- **Package Manager:** This project uses `uv` (not poetry) for dependency management
- **Python Version:** Requires Python >=3.9, <4.0
- **Installation:** Run `make install` to set up the environment and pre-commit hooks
- **Virtual Environment:** Always activate `.venv` before running any commands

### Common Commands:
```bash
# Install environment and pre-commit hooks
make install

# Run code quality checks
make check

# Run tests
make test

# Build documentation and serve locally
make docs

# Build wheel file
make build
```

---

## 5. Data Patterns

- **Data Formats:** The data in this repository is either in PG-JSON or JSON-LD format. In particular, we use the **Croissant JSON-LD format** for datasets.

- **JSON-LD Format:** JSON-LD (JSON for Linking Data) is a method of encoding linked data using JSON. Key elements:
  - `@context`: Defines the vocabulary and mappings
  - `@type`: Specifies the type of the entity
  - `@id`: Unique identifier for the entity
  - Properties follow schema.org and Croissant vocabulary

- **PG-JSON Format:** Property Graph JSON represents graphs with nodes and edges:
  - **Nodes**: Entities with IDs, labels, and properties
  - **Edges**: Relationships between nodes with type, source, target, and properties
  - Used for graph database storage and analysis

- **Dataset Structure:** Datasets contain metadata including:
  - Dataset identifier (hash-based UUID)
  - Dataset name and description
  - Data sources and locations (distribution array)
  - Schema information (recordSet)
  - Profile information (after profiling)

- **Analytical Patterns:** Query patterns that define:
  - Target dataset(s)
  - Query operations and transformations
  - Output specifications

- **Utilities for Data Validation and Conversion (json_tools):**
  - `validate_jsonld(...)`: Validates JSON-LD documents (strict and non-strict modes)
  - `convert_jsonld_to_pgjson(...)`: Transforms JSON-LD documents into Property Graph JSON for graph databases

- **Temporary Storage Locations:**
  - Raw datasets: `dmm_api/data/`
  - Query results: `dmm_api/data/results/USER_<id>_<timestamp>/`

---

## 6. Workflows

### Critical Pre-Workflow Check:
- **Always check and confirm the current working directory before running any bash command or referencing files.** This is a top priority for all agents. It ensures that all file paths and commands are executed in the correct context and prevents path-related errors. If a command or script fails, verify the working directory and adjust as needed before retrying. This instruction overrides other workflow steps if there is any ambiguity about the working directory.

### Development Workflow:
1. **Setup:** Ensure `.venv` is activated
2. **Code Changes:** Make modifications to Python files in `dmm_api/`
3. **Pre-Commit:** Pre-commit hooks will run automatically on commit (ruff linting/formatting, YAML/JSON validation, etc.)
4. **Testing:** Run `make test` to execute pytest suite
5. **Documentation:** Update docs in `docs/` if needed, run `make docs` to preview
6. **Commit:** Use `[BOT]` prefix in commit messages for automated commits

### API Development Workflow:
1. **Start API:** Run `python dmm_api/main.py` or `uvicorn dmm_api.main:app --host 0.0.0.0 --port 5000`
2. **Test Endpoints:** Use curl or access `/api/v1/docs` for Swagger UI
3. **Verify Changes:** Check endpoint responses and data persistence


### UUID v4 Generation for Code and Tests

When you need to generate UUID v4 values for code or tests, always use the following bash command:

```bash
uuidgen
```

Copy the output directly into the code to ensure valid UUID v4 values. This guarantees compliance with UUID v4 requirements in all validation and conversion logic.


### Editing Files
When editing files, always ensure that you maintain proper formatting and structure. First check the structure of the file to understand where your changes fit in. After making changes, inspect indentation, check again to ensure no formatting or structural issues were. Avoid inserting new content at the top of files unless absolutely necessary. Always place new content in the appropriate section of the file.


---

## 7. Code Quality & Testing

- **Pre-Commit Hooks:** Configured in `.pre-commit-config.yaml`
  - Ruff (linting and formatting)
  - YAML/JSON validation
  - Trailing whitespace and EOF fixes
  - Merge conflict checks

- **Testing:** Pytest-based test suite in `tests/`
  - Run with `make test` or `uv run python -m pytest`
  - Tests include dataset operations, profiling, and query execution

- **Code Quality:** Run `make check` to verify:
  - Lock file consistency
  - Pre-commit hook compliance

---

## 8. API Documentation

- **API Docs:** Available at `/api/v1/docs` (Swagger UI) when server is running
- **ReDoc:** Available at `/api/v1/redoc` for alternative documentation view
- **MkDocs:** Project documentation in `docs/` directory
  - Build and serve with `make docs`
  - Test build with `make docs-test`

### API Endpoints:
- GET `/api/v1/dataset` — List all datasets
- GET `/api/v1/dataset/{dataset_id}` — Retrieve a specific dataset by ID
- POST `/api/v1/dataset/register` — Register a new dataset (JSON-LD validation expected)
- PUT `/api/v1/dataset/load` — Move a dataset from the scratchpad into persistent storage
- PUT `/api/v1/dataset/update` — Update an existing dataset after profiling (attach profile)
- POST `/api/v1/polyglot/query` — Execute analytical pattern queries

- Notes:
  - Root API landing: GET `/api/v1` (returns available endpoints summary)

### Utilities Documentation:
- **Utilities README:** `dmm_api/utilities/README.md` - Guide to `json_tools` usage

---

## 9. What to Implement Next (Safe, Small Wins)

- **API Enhancements:**
  - Integrate JSON-LD validation into dataset registration endpoints
  - Add PG-JSON export endpoint for graph database integration
  - Enhance error handling and validation in API endpoints
  - Add API authentication and authorization

- **Testing & Quality:**
  - Add more comprehensive test coverage for utilities
  - Add integration tests for API with validation
  - Add performance tests for large dataset conversions

- **Features:**
  - Implement dataset search functionality
  - Add support for additional data formats beyond Croissant
  - Implement dataset versioning
  - Add support for asynchronous query processing
  - Add batch validation and conversion endpoints

- **Utilities Expansion:**
  - Add JSON Schema validation for Croissant format
  - Add PG-JSON to JSON-LD reverse conversion
  - Add graph visualization utilities
  - Add dataset comparison utilities

- **Documentation:**
  - Improve query result metadata documentation
  - Enhance documentation with more examples
  - Add tutorials for common workflows
- Implement dataset versioning
- Add support for asynchronous query processing
- Enhance documentation with more examples

---

## 10. References

- **Project Repository:** https://github.com/datagems-eosc/dmm-api
- **UV Documentation:** https://docs.astral.sh/uv/
- **FastAPI Documentation:** https://fastapi.tiangolo.com/
- **DuckDB Documentation:** https://duckdb.org/docs/
- **Croissant Format:** JSON-LD format for dataset metadata
- **MkDocs Material:** https://squidfunk.github.io/mkdocs-material/
